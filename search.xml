<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python Spider 3：文本解析]]></title>
    <url>%2Fpython-spider-3.html</url>
    <content type="text"><![CDATA[在上一节中，我们学会了使用requests库从网上请求数据，但是我们发现请求的数据很多时候是html页面，页面中有用的内容往往只有其中的一小部分，所以我们就需要在很大的文本（通常是html）中提取出对我们最有用的内容 在这一小节，我们会了解到如何解析json、如何使用正则表达式解析文本，以及使用xpath和css选择器解析html文档 jsonjson文本数据的格式1234567891011[&#123; "id": 1, "name": "FerlySky", "admin": true, "hobby": ["编程"]&#125;,&#123; "id": 2, "name": "cxk", "admin": false, "hobby": ["唱", "跳", "Rap", "篮球"]&#125;] json文本中，数据以键值对的形式出现，key值必须用双引号引起，value的值根据类型不同，有以下几种： null：只有null一种 数值类型：包括整数和小数 布尔类型：true和false 字符串：需要双引号 数组：用[]包裹，其中可以是任意value的类型，多个value之间用逗号分割 对象：用{}包裹，内部可以放多个键值对，用逗号分割 整个json的顶层必须是对象或者数组 Python与JsonJson反序列化我们获取到json文本之后，希望获得某个（某些）key值对应的value。这时我们通常需要用到python的官方json库 12345678910111213141516171819202122232425import jsontxt = '''[&#123; "id": 1, "name": "FerlySky", "admin": true, "hobby": ["编程"]&#125;,&#123; "id": 2, "name": "cxk", "admin": false, "hobby": ["唱", "跳", "Rap", "篮球"]&#125;]'''obj = json.loads(txt)cobj = obj[1]cname = cobj["name"]chobby = cobj["hobby"]print("name:", cname)print("hobby:"， end=' ')for hobby in chobby: print(hobby, end=' ') 其中，对象类型会被对应成python的dict，null会被对应成None Json序列化当然，我们有时候也有可能将python的对象转换成json字符串，我们可以使用json的dumps方法 123456789import jsoncxk = &#123; 'name': 'cxk', 'hobby': ['唱', '跳', 'rap', '篮球'], 'man': False&#125;print(json.dumps(cxk, ensure_ascii=False)) 正则表达式正则表达式通常被用来检索、替换那些符合某个模式(规则)的文本。 正则表达式语法常见语法： 字符 描述 普通字符 包括所有大写和小写字母、所有数字、所有标点符号和一些其他符号 转义字符 以斜线\开头的有特殊含义的字符，包括\t \r \n \\ 等，当要匹配特殊字符时需要在字符前加\ * 匹配前面的子表达式零次或多次 + 匹配前面的子表达式一次或多次 ? 匹配前面的子表达式零次或一次 {n} n 是一个非负整数。匹配确定的 n 次 {n,} 至少匹配n 次 {n,m} 最少匹配 n 次且最多匹配 m 次 [] 匹配中括号中的任意一个字符，[0-9] [a-zA-Z] ^ 匹配输入字符串开始的位置 $ 匹配输入字符串结尾的位置 . 匹配除换行符 \n 之外的任何单字符 详见：正则表达式 – 语法 | 菜鸟教程 万能匹配：(.*?) python中的正则表达式python为我们提供了内置的正则表达式功能，在re库中1234567891011121314151617181920212223242526import rehtml = '''&lt;html&gt;&lt;head&gt; &lt;title&gt;FsBlog&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;ol&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt; &lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;'''title = re.search('&lt;title&gt;(.*?)&lt;/title&gt;', html).group(1)lis = re.findall('&lt;li&gt;(.*?)&lt;/li&gt;', html)print('title:', title)for li in lis: print('li:', li) 要获取第一个匹配到的字符串，则可以使用re.search(pattern, string, flags), 其中pattern为正则表达式，string为要被匹配的源字符串，flags为匹配参数，如果希望可以匹配多行字符串，则可以在flags处传入re.S，获得完整的匹配内容需要调用group(0)，如果想获得第几个子匹配（小括号内部匹配的内容），则可以使用group(n) 要匹配多个字符串，则可以使用re.findall(pattern, string, flags)，结果返回一个数组 Xpath解析上面的正则表达式是基于文本匹配的方式获取需要的内容，它的效率比较低，而xpath方法基于dom树的解析方式效率会高很多，而且实现起来更加简单 xpath解析有局限性，只能解析xml或者类似的html等标记文档 接下来我将简单介绍xpath的术语以及其语法，当然你也可以查看XPath 教程 | 菜鸟教程学习更加详细的xpath知识 Xpath术语123456&lt;html lang="zh"&gt; &lt;head&gt; &lt;title&gt;FsBlog&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;/body&gt;&lt;/html&gt; 以上是一个简单的html结构，其中： 节点：每个元素、属性、文本、命名空间、注释都是一个节点 根节点：文档的最外层节点， 父节点：html是head和body的父节点, head是title的父节点 子节点：head和body是html的子节点，title是head的子节点 同胞节点：head和body是同胞节点 先辈节点：父节点和父节点的父节点，html和head都是title的先辈节点 后辈节点：子节点和子节点的子节点，head、title和body都是html的后辈节点 Xpath语法 表达式 描述 节点名 选取此节点的所有子节点 / 从根节点选取 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 . 选取当前节点 .. 选取当前节点的父节点 @ 选取属性 如果想获取html节点下lang的值，可以用 //html/@lang 如果想获取title节点的内容，可以用 //title/text() Python的Xpath解析（lxml）要使用python解析xpath，需要安装一个第三方库：lxml pip install lxml 1234567891011121314151617from lxml import etreehtml = '''&lt;html lang="zh"&gt; &lt;head&gt; &lt;title&gt;FsBlog&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;/body&gt;&lt;/html&gt;'''tree = etree.HTML(html)title = tree.xpath('//title/text()')[0]lang = tree.xpath('//html/@lang')[0]print('title:', title)print('lang:', lang) 先从lxml库中导出etree，使用etree.HTML()函数创建dom树对象 调用tree的xpath(xpath)方法，获取所有匹配到的内容，返回一个数组 使用CSS选择器解析这种方式是采用css选择元素的方式一致，非常适合对css有所了解的人 css选择器语法 选择器 描述 .class 类选择器 #id id选择器 * 全选 element 匹配所有指定元素名的元素 a, b 匹配a选择器或b选择器中的任意一个即可匹配 a b 匹配a的后辈中的b a&gt;b 匹配a的子元素b a+b 匹配a紧跟着的兄弟b [attr] 匹配具有特定属性的元素 [attr=val] 匹配具有指定属性并且值为指定值的元素 详细的css选择器，查看CSS 选择器 | 菜鸟教程 注意：伪类 :link :active :hover :focus :visited 并不能匹配元素 Python使用css选择器解析文档（BeautifulSoup）在python中使用css选择器解析文档需要安装一个第三方库：BeautifulSouppip install BeautifulSoup4 1234567891011121314151617from bs4 import BeautifulSouphtml = '''&lt;html lang="zh"&gt; &lt;head&gt; &lt;title&gt;FsBlog&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;/body&gt;&lt;/html&gt;'''soup = BeautifulSoup(html, 'html.parser')title = soup.title.textlang = soup.html['lang']print('title:', title)print('lang:', lang) 其中BeautifulSoup构造方法的第二个参数是解析器，可选的参数及优缺点如下： BeautifulSoup官方中文文档]]></content>
      <categories>
        <category>网络爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>网络爬虫</tag>
        <tag>文本解析</tag>
        <tag>json</tag>
        <tag>xpath</tag>
        <tag>正则表达式</tag>
        <tag>css selector</tag>
        <tag>lxml</tag>
        <tag>BeautifulSoup4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Spider 2: 网络请求]]></title>
    <url>%2Fpython-spider-2.html</url>
    <content type="text"><![CDATA[要想分析到有用的数据，我们首先要先获取到网络资源，其中最主要的就是网页和json 浏览器在获取资源时，实际上是向浏览器发送了http请求 这一章中我们会了解如何手动发送http请求获取资源，以及如何用已有的框架（requests）获取资源 建立Tcp连接模拟Http请求http请求的本质是一个tcp请求，只是传输数据的格式比较特殊。 Http请求报文解析http请求报文由3部分组成：请求行，请求头，请求体。其中某些请求方式可能没有请求体。 下面是一个http post的请求报文：12345678910111213141516POST /weapi/search/suggest/web?csrf_token= HTTP/1.1Host: music.163.comConnection: keep-aliveContent-Length: 394Pragma: no-cacheCache-Control: no-cacheOrigin: https://music.163.comUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36Content-Type: application/x-www-form-urlencodedAccept: */*Referer: https://music.163.com/Accept-Encoding: gzip, deflate, brAccept-Language: zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7Cookie: starttime=;params=tZA39GIzuYfZGwabdxUe3OtRtEj8BqfGyk95pyloHT3iIsxUfDmEC60V1rxn0DHfvKVnXrACICP6ZV7a8wQC74RO%2BHPxDM%2FlwiYU%2B%2BwS%2Bf8%3D&amp;encSecKey=8bf98f9a88662ac9606ff2c9336a23f6d9e7413cb3c6c9c2ef56eeec2e814ead675a032c9905b21526bd8ed59aa0862d0114d78629f954f47276cb3f2a9987faf49add6b5c289776c23329cada89afbb51e6e5c756c637be364a695013c1029da680894766a234656a667991cd580c8cd047160de48e0ff9982437826d374524 第一行为请求行：POST /weapi/search/suggest/web?csrf_token= HTTP/1.1，由三部分组成，请求方式，请求路径和协议版本 请求方式：爬虫最常见的请求方式为GET方式和POST方式，除此之外还有DELETE，PUT，HEAD等 请求路径：请求路径是一个uri，是相对服务器项目根路径下的地址 协议版本：HTTP1.0和HTTP1.1，其中1.0每完成一次请求都会断开连接，而HTTP1.1会保持连接一段时间 从第二行开始，到空行结束的部分为请求头。表示请求时的一些请求通用参数，它的本质为一个Map，其中请求头的key有固定的含义，不区分大小写 第三部分为请求体，不同的请求类型，请求体的格式也不同，常见为key-value形式，用于表示参数，key和value之间用’=’连接，参数之间用’&amp;’连接 注意请求头和请求体之间有一个空行，请求体结束也需要一个空行 Http响应报文解析响应报文和请求报文一样，由响应行，响应头和响应体组成1234567891011121314151617181920212223HTTP/1.0 200 OKAccept-Ranges: bytesCache-Control: no-cacheContent-Length: 14615Content-Type: text/htmlDate: Mon, 15 Apr 2019 10:34:52 GMTEtag: "5c9c7bd5-3917"Last-Modified: Thu, 28 Mar 2019 07:46:29 GMTP3p: CP=" OTI DSP COR IVA OUR IND COM "Pragma: no-cacheServer: BWS/1.1Set-Cookie: BAIDUID=71A25E6D0916916E940A4FCA7B0D87C6:FG=1; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.comSet-Cookie: BIDUPSID=71A25E6D0916916E940A4FCA7B0D87C6; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.comSet-Cookie: PSTM=1555324492; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.comVary: Accept-EncodingX-Ua-Compatible: IE=Edge,chrome=1&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv="content-type" content="text/html;charset=utf-8"&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=Edge"&gt; &lt;link rel="dns-prefetch" href="//s1.bdstatic.com"/&gt; 响应行由三部分组成，协议版本，响应状态码，响应消息。其中响应消息和相应状态码通常相对应 HTTP状态码分类： 分类 分类描述 1xx 表示服务器收到了请求，需要请求者继续发送请求 2xx 表示操作成功 3xx 重定向，表示需要进一步操作 4xx 客户端错误，表示语法错误或无法完成请求 5xx 服务端错误，表示服务器在处理请求的过程中发生了错误 常见的状态码： 状态码 消息 描述 200 OK 请求成功 302 REDIRECT 暂时的重定向 400 BAD REQUEST 请求错误 404 NOT FOUND 资源未找到 500 Internal Server Error 服务器错误 响应头与请求头一样，都是有固定一样的键值参数 响应体就是与我们请求时的uri和参数的资源啦。 使用python发送http请求我们这里以请求百度的主页为例，看一下如何使用tcp来发送http请求12345678910111213141516171819202122232425import socketif __name__ == '__main__': client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 创建socket对象 client.connect(('www.baidu.com', 80)) # 连接百度服务器，http请求默认为80端口 request_line = 'GET / HTTP/1.0' # 请求行 request_header = 'HOST: www.baidu.com' # 请求头 request_body = '' # 请求体 # 拼接请求字符串 if len(request_body) &gt; 0: request_text = '\r\n'.join([request_line, request_header, request_body, '']) + '\r\n' else: request_text = '\r\n'.join([request_line, request_header]) + '\r\n\r\n' print(request_text) # 输出请求报文 client.send(request_text.encode()) # 发送请求报文 print('*' * 150) # 分割线 # 获取相应报文 data = b'' while True: rec_data = client.recv(1024) if rec_data: data += rec_data else: break; print(data.decode('utf-8')) Tcp方式的缺点在实际爬虫或其他http请求需求的时候，我们很少会用tcp方式发送http请求，因为它有很多缺点 需要手动构建请求路径uri，手动构建uri的请求参数，并且在需要时进行url编码 需要手动拼接请求头 传递参数或者上传文件（图片）等资源时需要手动处理 解析响应报文非常复杂且容易出错 很难使用代理服务器发送请求 requests由于Tcp发送http请求的方式有很多缺点，所以我们通常会使用一些http请求的框架来帮我们解决这些问题。而在python中，最常见的http请求库就是requests 由于requests是第三方库，所以我们在使用requests之前，需要先安装它：pip install requests 引入requests：import requests 快速入门：12345import requestsif __name__ == '__main__': resp = requests.get('https://www.baidu.com') print(resp.text) 发送请求发送不同的请求方式的请求，可以直接调用对应的方法即可123456response = requests.get('https://api.github.com/events')response = requests.post('http://httpbin.org/post', data = &#123;'key':'value'&#125;)response = requests.put('http://httpbin.org/put', data = &#123;'key':'value'&#125;)response = requests.delete('http://httpbin.org/delete')response = requests.head('http://httpbin.org/get')response = requests.options('http://httpbin.org/get') 由于post和put方法往往需要传递参数，可以直接传递data这个命名参数，类型为map类型 url参数有些时候我们需要在通过url传递参数，我们可以使用params参数123456params = &#123; 'ie': 'UTF-8', 'word': 'test'&#125;url = 'https://www.baidu.com/s'response = requests.get(url,params=params) 这时，我们get的真实url为：https://www.baidu.com/s?ie=UTF-8&amp;wd=test 自定义请求头在请求的时候，我们往往需要模拟Host或Origin请求头来进行模拟跨域操作，需要使用到headers123456headers = &#123; 'Accept': '*/*', 'Host': 'www.baidu.com'&#125;url = 'https://www.baidu.com/'response = requests.get(url, headers=headers) 上传文件有些时候我们可能需要上传某些文件，比如修改头像等，我们可以使用files参数123files = &#123;'file': open('avatar.png', 'rb')&#125;url = 'http://httpbin.org/post'response = requests.post(url, files=files) 获取响应123456print('text', response.text) # 获取响应文本，类型为strprint('content', response.content) # 获取响应内容，类型为bytesprint('json', response.json()) # 将响应内容当作json字符串解析print('status code', response.status_code) # 获取响应状态码print('encoding', response.encoding) # 获取编码格式print('headers', response.headers) # 获取响应头 修改响应编码格式：response.encoding = &#39;UTF-8&#39;，先修改完编码格式，在通过response.text即可获取UTF-8（或其他）编码格式下的响应文本 流式请求有时我们请求的资源不一定都是文本类型，有可能是图片，视频，音乐等文件类型，这些文件通常比较大，虽然可以使用上面的普通方式，通过response.content获取二进制字符集，但是我们使用数据流的方式能更方便的处理大型文件 123456chunk_size = 1024 * 16 # 声明数据块大小response = requests.get('https://ferlysky.gitee.io/images/avatar.png', stream=True)with open('avatar.png', 'wb') as f: # 迭代数据块，并写入到文件中 for chunk in response.iter_content(chunk_size): f.write(chunk) 这里用到了文件io，我会在后面的章节进行讲解 代理爬虫时，我们往往会被网站的反爬机制限制ip访问，这时我们可以使用代理服务器来解决 12345proxies = &#123; "http": "http://10.10.1.10:3128", "https": "http://10.10.1.10:1080",&#125;requests.get("http://example.org", proxies=proxies) session在我们进行请求的时候，有时需要我们传递cookies(例如登录之后才能进行的操作) 手动cookies请求时携带cookies123456jar = requests.cookies.RequestsCookieJar()jar.set('name', 'FerlySky')jar.set('url', 'https://ferlysky.gitee.io')url = 'http://httpbin.org/cookies'response = requests.get(url, cookies=jar)print(response.json()) 获取响应cookies1print('cookies', response.cookies) 自动携带cookies每次都需要手动设置cookies，并且把响应的cookies拿出来再次设置到下次请求中实在是太麻烦了，requests库给我们提供了一种简单的方法，就是使用session对象 使用session对象，可以将我们上一次请求时响应的cookies保存下来，在下一次请求时发送出去 1234session = requests.session()response = session.get('http://www.baidu.com/')print(response.cookies['BDORZ']) session的操作和requests几乎完全一致]]></content>
      <categories>
        <category>网络爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>网络爬虫</tag>
        <tag>网络请求</tag>
        <tag>http</tag>
        <tag>requests</tag>
        <tag>tcp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Spider 1: 概述]]></title>
    <url>%2Fpython-spider-1.html</url>
    <content type="text"><![CDATA[随着大数据时代的到来，人们对数据资源的需求越来越多，而爬虫是一种很好的自动采集数据的手段。 网络爬虫 网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。 爬虫的分类及应用爬虫按照系统结构和实现技术大致可以分为：通用网络爬虫、聚焦网络爬虫 、增量式网络爬虫和深层网络爬虫。 通用网络爬虫：通用爬虫的目标是尽可能大的网络覆盖。由于其数据涵盖范围较广，很多时候并不能获取到比较专业的资源。像百度谷歌等搜索引擎就是典型的通用爬虫。 聚焦网络爬虫：它根据要爬取的目标，有选择性的获取互联网上的资源，其结果更具有针对性。 增量式网络爬虫：只爬取新产生的或者发生变化的网页，不爬取已经获取过的资源。可以减小时间、空间以及带宽的消耗，但是实现起来相对复杂 深层网络爬虫：爬取某些非表面显示的资源，比如需要登录账号才能查看的资源。 爬虫的工作流程 首先选取一部分精心挑选的种子URL； 将这些URL放入待抓取URL队列； 从待抓取URL队列中取出待抓取在URL，解析DNS，并且得到主机的ip，并将URL对应的网页下载下来，存储进已下载网页库中。此外，将这些URL放进已抓取URL队列。 分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环。]]></content>
      <categories>
        <category>网络爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>网络爬虫</tag>
        <tag>概述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法 1: 动态规划]]></title>
    <url>%2Falgorithm-1.html</url>
    <content type="text"><![CDATA[动态规划是一种通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。 动态规划常常适用于有重叠子问题和最优子结构性质的问题。 基本思想若要解一个给定问题，我们需要解其不同部分（即子问题），再合并子问题的解以得出原问题的解。通常许多子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量：一旦某个给定子问题的解已经算出，则将其记忆化存储，以便下次需要同一个子问题解之时直接查表。这种做法在重复子问题的数目关于输入的规模呈指数增长时特别有用。 动态规划的核心是：记忆已经解决的子问题的解 问题特征动态规划的基本特征：问题的最优解需要使用到其子问题的最优解 解题步骤 分析最优解的结构：将问题划分成两个子问题，如果原问题获得最优解，子问题也应该是最优解。 建立递推关系 计算最优解 例题最长回文子串问题 问题：给定一个字符串s，找到s中最长的回文子串。（选自Leetcode #5） 分析解结构： 如果一个字符串是回文串，那么它去掉收尾后也应该是回文串。那么我们就可以讲问题转化为先找到一个小的回文串，然后在判断该回文串在原字符串中前后两个字符是否相同，如果相同，将该子回文串的前后都添加这个字符，新的字符串仍是回文串。 建立递推关系： 设 i, j为字符串s的某一子字符串s[i:j]的开始索引和结束索引，i &lt;= j，我们可以使用一个二维的bool数组 b 来记录位置 b[i][j] 对应的子字符串 s[i:j] 是否为回文串。 如果 i == j, 说明s[i:j]为单个字符，必定是一个回文串 如果 j - i == 1, 说明s[i:j]是一个长度为2的字符串，如果 s[i] == s[j], 则s[i:j]是一个回文串 如果 i, j 不满足前两条，说明s[i:j]是一个长度大于等于3的字符串，它可以由它去掉两端的字串s[i+1:j-1]，拼接两个字符 s[i], s[j] 来解决，如果 s[i+1:j-1] 是一个回文串，同时 s[i] == s[j], 则s[i:j]是一个回文串 用公式表示为： 我们以 s = “atrattardd” 为例，画出二维数组： 我们可以清晰的发现，如果一个位置为0，那么它右上方的位置一定为0，并且要获取最长回文子串，只需要找到矩阵中，离右上角最近的1的位置即可。对于该示例而言，最长回文子串的位置为s[2:7] = “rattar” 算法实现123456789101112131415161718192021222324252627282930class Solution &#123; public String longestPalindrome(String s) &#123; int length = s.length(); char[] chars = s.toCharArray(); // 用来记录对应子串是否为回文子串 boolean[][] bmap = new boolean[length][length]; // 用来记录长度的最大值 int max = 0; int start = 0; int end = 0; // 为了保证在计算某一位置之前，其最下方位置已经被计算并保存过，所以这里i从下往上遍历 for (int i = length - 1; i &gt;= 0; i--) &#123; for (int j = i;j &lt; length; j++) &#123; // &amp;&amp; j &lt; length if (i == j) &#123; bmap[i][j] = true; &#125; else if (j - i == 1) &#123; bmap[i][j] = chars[i] == chars[j]; &#125; else &#123; bmap[i][j] = bmap[i+1][j-1] &amp;&amp; chars[i] == chars[j]; &#125; if (bmap[i][j] &amp;&amp; j - i + 1 &gt; max) &#123; max = j - i + 1; start = i; end = j; &#125; &#125; &#125; return s.substring(start, end + 1); &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>算法</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flutter Plugins 1: Fluttertoast]]></title>
    <url>%2Fflutter-plugin-1.html</url>
    <content type="text"><![CDATA[Fluttertoast 是一个弹出toast的第三方框架 安装12dependencies: fluttertoast: ^3.0.3 获取最新版本号：https://pub.flutter-io.cn/packages/fluttertoast 使用方法导包1import 'package:fluttertoast/fluttertoast'; 弹出toast123456789Fluttertoast.showToast( msg: "This is Center Short Toast", toastLength: Toast.LENGTH_SHORT, gravity: ToastGravity.CENTER, timeInSecForIos: 1, backgroundColor: Colors.red, textColor: Colors.white, fontSize: 16.0); 参数: 参数 描述 msg String，要弹出的消息，非空必须 toastLength Toast.LENGTH_SHORT、 Toast.LENGTH_LONG gravity ToastGravity，toast显示的位置，ToastGravity.TOP、ToastGravity.CENTER、ToastGravity.BOTTOM timeInSecForIos int，仅限ios平台 backgroundColor Color，背景颜色 textColor Color，文字颜色 fontSize float，文字大小 取消弹出1Fluttertoast.cancel();]]></content>
      <categories>
        <category>Flutter</category>
        <category>Flutter Plugins</category>
      </categories>
      <tags>
        <tag>Flutter</tag>
        <tag>Flutter Plugin</tag>
      </tags>
  </entry>
</search>
